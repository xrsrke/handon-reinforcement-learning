{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d390ee4-c36d-468c-abb8-f31d3c692683",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b97434-5494-45c1-9135-e252c414b65b",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd101463-e6c1-4b2a-b2c3-dcbb678ca0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6474ae4-680e-4117-87ed-b6fc94a0054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "std = torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ee15e7-17d2-43db-8b7f-1535061f8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Normal(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c85236-a158-444d-9904-c1e6d7691665",
   "metadata": {},
   "source": [
    "Sample an action from the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4d4668-c719-43ca-88a3-7c23db1df23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfeda10-0fc9-4f06-b833-0b1e850324cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6733, -0.2301,  1.6005])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7eb64-6d81-4e47-ac56-e4ebe0d382cb",
   "metadata": {},
   "source": [
    "Calculate the log probability of the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7dae2b-c86f-487b-9138-8f6031b3796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa50bc04-23c3-40a8-8e12-fdd345528d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1456, -0.9454, -2.1998])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf9277-0c01-44e5-944a-9047ddaf8a92",
   "metadata": {},
   "source": [
    "# 1. Soft Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d43bae-e94e-4a6c-af2d-46eeabc4b212",
   "metadata": {},
   "source": [
    "### 1.1 Policy evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78996731-51af-4ecd-ad5b-39753ff418d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f486c-c11c-49d3-9d4f-20cb69ce778e",
   "metadata": {},
   "source": [
    "Write a pseudo-code to update the Q function. According to the policy evaluation step's equation:\n",
    "\n",
    "$\\mathcal{T}^\\pi Q\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right) \\triangleq r\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p}\\left[V\\left(\\mathbf{s}_{t+1}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80821530-296d-44e4-be74-cd3e0d907264",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "- `n_steps` is the number of time steps that an agent interacts in an episode\n",
    "- `states` and `actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae278e7-a8d6-4779-8920-2d296928336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "    # Compute the reward for the current time step\n",
    "    reward = compute_reward(states[t], actions[t])\n",
    "    \n",
    "    # Compute the expected value of the next state\n",
    "    expected_value = value_function(states[t+1]).mean()\n",
    "    \n",
    "    # Update the value function using the modified Bellman backup operator\n",
    "    Q[t] = reward + gamma * expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b3dff-dc43-446f-8d1d-8ff61b66051d",
   "metadata": {},
   "source": [
    "### 1.2 Policy improvement step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465fb91-0112-4883-8a4d-c082cc6df94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a0ddb-701f-4679-a5c2-7e169954d6b8",
   "metadata": {},
   "source": [
    "$\\pi_{\\text {new }}=\\arg \\min _{\\pi^{\\prime} \\in \\Pi} D_{K L}\\left(\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{s}_t\\right) \\| \\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d752ca5-bc17-47b6-8e31-16fb017568b6",
   "metadata": {},
   "source": [
    "In the context of soft actor-critic. Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbef50-00b5-4bdf-b1b5-ce13b736b6c8",
   "metadata": {},
   "source": [
    "Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c2694-81d1-4477-aba7-a4cae11baf3a",
   "metadata": {},
   "source": [
    "**1. What does the equation do?**\n",
    "\n",
    "**Answer**: Minimize the difference between two probability distributions: $\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{S}_t\\right)$ and $\\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}\\left(\\mathbf{s}_t\\right)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236b98b-2671-4271-8687-d9581a92552b",
   "metadata": {},
   "source": [
    "`Notations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b67934-de8e-45ea-bc1c-0566b668555d",
   "metadata": {},
   "source": [
    "**2. What does $\\pi^{\\prime} \\in \\Pi$ mean?**\n",
    "\n",
    "**Answer**: Restricts the possible new policies to a set of policies \n",
    "\n",
    "\n",
    "**3. What does $Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)$ mean? Why need it?**\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- A scalar value that is used to normalize the distribution represented by the exponentiated Q-function\n",
    "- Ensures that the distribution sums to 1 over all actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067c96e-dbb7-40d7-84de-53543b8fddc2",
   "metadata": {},
   "source": [
    "`Q-Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac987b18-736a-472f-95a8-6d9b7b394625",
   "metadata": {},
   "source": [
    "**4. What does $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ mean?**\n",
    "\n",
    "**Answer**: It's a function of actions. So it takes in an action as an input and produces a scalar output.\n",
    "\n",
    "\n",
    "\n",
    "**5. What does $\\cdot$ $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ represent?**\n",
    "\n",
    "**Answer**: The dot indicates that the action component of the state-action pair has not been specified yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c1735-e9ae-48e6-ac2c-ba9ee2164644",
   "metadata": {},
   "source": [
    "**6. Why is the old $\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t,{\\cdot}\\right)\\right)$ the target for the new policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc66ea-504f-4080-b3a8-ef2f8b2b9ad0",
   "metadata": {},
   "source": [
    "**7. Why is the exponential of $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ taken in this equation?**\n",
    "\n",
    "**Answer**: Ensure that the resulting distribution is a valid probability distribution, since the exponential function maps all real numbers to positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c73a4-937e-4dd2-9ea3-cdb5bc3c2376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c802c87-61f1-40a8-8384-66df905527ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32740f44-968a-4921-91e2-19cf8995bbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59205cd-5278-49d2-8a63-deeae393ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92fcad-6539-4d0b-a42e-de7f576a3480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b06bca3-3f57-4d33-acf0-3a45767d4067",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f6806-e724-43f6-9672-e1389dbb5460",
   "metadata": {
    "tags": []
   },
   "source": [
    "Policy improvement step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6cf15-775d-4f6a-9df5-ee0ea4a8e718",
   "metadata": {},
   "source": [
    "$\\pi_{\\text {new }}=\\arg \\min _{\\pi^{\\prime} \\in \\Pi} D_{K L}\\left(\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{s}_t\\right) \\| \\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c435ab-8200-4c5d-88e6-e453ccad2ad6",
   "metadata": {},
   "source": [
    "Wha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
