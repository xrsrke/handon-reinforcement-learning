{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1597aea-3bbe-4df8-9f05-cbb3b821af6c",
   "metadata": {},
   "source": [
    "In the equation above, $\\hat{A}_t$ is an estimator for the advantage function at timestep $t$. The advantage function is a measure of how much better a particular action is compared to the average action at a given state. It is a common quantity used in reinforcement learning algorithms to guide the optimization of the policy.\n",
    "\n",
    "The advantage estimator is defined in terms of the sequence of rewards $r_t$ received by the agent and the value function $V(s_t)$, which estimates the expected return starting from state $s_t$. The value function is typically learned as part of the reinforcement learning algorithm.\n",
    "\n",
    "The term $\\delta_t$ represents the difference between the expected return from the current state and the predicted value of the current state, given by the expression $r_t + \\gamma V(s_{t+1}) - V(s_t)$. The term $\\gamma$ is the discount factor, which determines the importance of future rewards relative to current rewards.\n",
    "\n",
    "The advantage estimator is defined as a sum over all the $\\delta_t$ terms, with each term weighted by the factor $(\\gamma \\lambda)^{T-t+1}$. The term $\\lambda$ is a free parameter that determines the amount of weight given to future rewards. When $\\lambda=1$, the advantage estimator reduces to the well-known generalized advantage estimation (GAE) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ffb05-c68a-4121-b22e-c2c8a19faa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77829b71-e1cd-43ad-b915-1b8f737431db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592caeb-335e-4cb2-ba13-d77da53b17a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94ced5cc-a7af-43cc-816b-72b61f86d128",
   "metadata": {},
   "source": [
    "In the equation above, $\\hat{A}t$ is an estimator for the advantage at timestep $t$ within a given length-$T$ trajectory segment. The term $\\delta_t$ is the advantage at timestep $t$ and is defined as the difference between the reward received at timestep $t$, the expected value of the next state, and the expected value of the current state: $\\delta_t=r_t+\\gamma V\\left(s{t+1}\\right)-V\\left(s_t\\right)$.\n",
    "\n",
    "The term $(\\gamma \\lambda) \\delta_{t+1}$ is the weighted advantage at timestep $t+1$, where $\\gamma$ is the discount factor and $\\lambda$ is a parameter that determines the weighting of the advantages. The discount factor $\\gamma$ determines the importance of future rewards, with a value close to 1 implying that future rewards are important and a value close to 0 implying that only immediate rewards are important. The parameter $\\lambda$ determines how much the current advantage depends on future advantages. A value of $\\lambda = 1$ corresponds to the full importance of future advantages, while a value of $\\lambda = 0$ corresponds to no importance of future advantages.\n",
    "\n",
    "The term $(\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$ is the weighted advantage at the final timestep $T-1$ of the trajectory segment, where the exponent $(T-t+1)$ determines the weighting of this advantage.\n",
    "\n",
    "The full equation $\\hat{A}t=\\delta_t+(\\gamma \\lambda) \\delta{t+1}+\\cdots+\\cdots+(\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$ can be interpreted as a weighted sum of the advantages at each timestep, where the weighting of each advantage depends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b123405-ac16-40a2-8657-7c751d3b746d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5d4f6-4f10-472f-b5ff-0987438c5768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec88e14f-a1a5-4493-ae0f-dc1aeb927130",
   "metadata": {},
   "source": [
    "Imagine that you are trying to navigate a maze. You start at the entrance and have to find the exit. As you move through the maze, you come across forks in the road where you have to make a choice about which path to take. You don't know which path leads to the exit, so you have to explore and try different paths.\n",
    "\n",
    "The term $\\delta_t=r_t+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_t\\right)$ represents the difference between the reward you receive at each step and the value of being in that state. This term is important because it helps the agent determine whether a particular path is good or bad. If the reward for taking a particular path is high, then that path is likely to be good. If the value of being in a particular state is low, then that state is likely to be bad.\n",
    "\n",
    "The term $\\hat{A}t=\\delta_t+(\\gamma \\lambda) \\delta{t+1}+\\cdots+\\cdots+(\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$ represents the overall value of the path that the agent has taken. This term is important because it helps the agent determine which paths are the most valuable. If a path has a high value, then it is likely to be a good path to take.\n",
    "\n",
    "The purpose of this equation in PPO is to help the agent learn which paths are the most valuable so that it can navigate the maze more efficiently. By using this equation to estimate the value of each path, the agent can learn which paths are the most valuable and avoid paths that are not as valuable. This helps the agent find the exit to the maze more quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb7597-18af-41af-8a77-48d47c3d19f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
