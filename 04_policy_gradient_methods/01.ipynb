{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]\n",
    "hidden_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_observations, hidden_size),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(hidden_size, n_actions),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0009\n",
    "optimizier = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02845098,  0.02649889,  0.0030152 ,  0.02776563], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(torch.from_numpy(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_action(preds):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = select_random_action(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "None (<class 'NoneType'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_observation, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    215\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    216\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    217\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:132\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    131\u001b[0m     err_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action), err_msg\n\u001b[1;32m    133\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: None (<class 'NoneType'>) invalid"
     ]
    }
   ],
   "source": [
    "new_observation, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    n_rewards = len(rewards)\n",
    "    timesteps = torch.arange(0, n_rewards)\n",
    "    \n",
    "    # calculate the discount for each time step\n",
    "    discount = torch.pow(discount_factor, timesteps)\n",
    "    discounted_rewards = discount * rewards\n",
    "    \n",
    "    # normalize\n",
    "    discounted_rewards /= discounted_rewards.max()\n",
    "\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3401, 0.6734, 1.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards(rewards, discount_factor=0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\gamma_t * G_t * \\log \\pi_s(a \\mid \\theta)$\n",
    "\n",
    "$G_t$: is called the sum of all rewards until time $t$\n",
    "- $G_t=r_t+r_{t+1} \\ldots+r_{T-1}+r_T$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(selected_actions, discounted_rewards):\n",
    "    return -1 * torch.sum(discounted_rewards * torch.log(selected_actions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(MAX_EPISODES):\n",
    "#     observation, _ = env.reset()\n",
    "#     observation = torch.from_numpy(observation)\n",
    "#     done= False\n",
    "#     transitions = []\n",
    "    \n",
    "#     for t in range(MAX_DUR):\n",
    "#         predicted_actions = model(observation).float()\n",
    "#         action = torch.multinomial(predicted_actions, num_samples=1)[0].item()\n",
    "        \n",
    "#         new_observation, reward, done, truncated, info = env.step(action)\n",
    "#         transitions.append((observation.numpy(), action, reward, new_observation))\n",
    "        \n",
    "#         if done: break\n",
    "        \n",
    "#         scores.append(len(transitions))\n",
    "        \n",
    "#         reward_batch = torch.tensor([reward for (observation, action, reward) in transitions]).flip(dims=(0,))\n",
    "#         discounted_reward = discount_rewards(reward_batch, discount_factor=0.99)\n",
    "        \n",
    "#         observation_batch = torch.tensor([observation for (observation, action, reward) in transitions])\n",
    "#         action_batch = torch.tensor([action for (observation, action, reward) in transitions])\n",
    "        \n",
    "#         predicted_action_batch = model(observation_batch)\n",
    "        \n",
    "#         predicted_action_batch = predicted_action_batch.gather(\n",
    "#             dim=1,\n",
    "#             index=action_batch.long().view(-1,1)\n",
    "#         ).squeeze()\n",
    "        \n",
    "#         loss = loss_func(predicted_action_batch, discounted_reward)\n",
    "#         losses.append(loss.detach().numpy())\n",
    "#         optimizier.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizier.step()\n",
    "        \n",
    "#         observation = torch.from_numpy(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 1000\n",
    "\n",
    "discount_factor = 0.99\n",
    "scores = []\n",
    "transitions = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    predicted_action = model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transitions(model, env):\n",
    "    transitions = []\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        predicted_action = model(torch.from_numpy(state))\n",
    "        action = torch.argmax(predicted_action, dim=-1)\n",
    "        next_state, reward, done, truncated, info = env.step(action.item())\n",
    "        \n",
    "        transitions.append((\n",
    "            state, action, reward, next_state\n",
    "        ))\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "        state = next_state\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transitions(transitions):\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    for transition in transitions:\n",
    "        state, action, reward, _ = transition\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# make prediction over a batch of states\u001b[39;00m\n\u001b[1;32m     15\u001b[0m predicted_actions \u001b[39m=\u001b[39m model(states)\n\u001b[0;32m---> 16\u001b[0m selected_actions \u001b[39m=\u001b[39m predicted_actions(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(predicted_actions)), actions)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m loss_func(selected_actions, discounted_rewards)\n\u001b[1;32m     19\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "N_EPISODE = 1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "losses = []\n",
    "\n",
    "for episode in range(N_EPISODE):\n",
    "    transitions = generate_transitions(model=model, env=env)\n",
    "    states, actions, reward = extract_transitions(transitions)\n",
    "    discounted_rewards = discount_rewards(rewards, discount_factor=DISCOUNT_FACTOR)\n",
    "    \n",
    "    # convert to torch tensor\n",
    "    states = torch.tensor(states)\n",
    "    \n",
    "    # make prediction over a batch of states\n",
    "    predicted_actions = model(states)\n",
    "    selected_actions = predicted_actions(range(len(predicted_actions)), actions)\n",
    "    \n",
    "    loss = loss_func(selected_actions, discounted_rewards)\n",
    "    losses.append(loss.detach().numpy())\n",
    "    \n",
    "    optimizier.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizier.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
