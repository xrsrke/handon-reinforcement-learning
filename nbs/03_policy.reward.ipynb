{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp policy.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    n_rewards = len(rewards)\n",
    "    timesteps = torch.arange(0, n_rewards)\n",
    "    \n",
    "    # calculate the discount for each time step\n",
    "    discounts = torch.pow(discount_factor, timesteps)\n",
    "    discounted_rewards = discounts * rewards\n",
    "    \n",
    "    # normalize\n",
    "    discounted_rewards /= discounted_rewards.max()\n",
    "\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the discounted return of a timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_discounted_return_time_step(rewards, timestep, discount_factor):\n",
    "    discounted_return = torch.zeros(1)\n",
    "    current_and_future_rewards = rewards[timestep:]\n",
    "    for i, reward in enumerate(current_and_future_rewards):\n",
    "        discounted_return += (discount_factor ** i) * reward\n",
    "    \n",
    "    return discounted_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test_hello():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.7419])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_time_step(rewards, timestep=2, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_discounted_return_each_timestep(rewards, discount_factor):\n",
    "    n_reward = len(rewards)\n",
    "    discounted_returns = torch.zeros(n_reward).float()\n",
    "    \n",
    "    for i in range(n_reward):\n",
    "        discounted_return = 0\n",
    "        \n",
    "        for k, reward in enumerate(rewards[i:]):\n",
    "            discounted_return += (discount_factor**k) * reward\n",
    "        \n",
    "        discounted_returns[i] = discounted_return\n",
    "    \n",
    "    return discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [torch.tensor(5), torch.tensor(2), torch.tensor(3), torch.tensor(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.8015,  8.8904,  6.9600,  4.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_each_timestep(rewards, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_discounted_return_an_episode(rewards, discount_factor=0.99):\n",
    "    total_returns = torch.zeros(1)\n",
    "    discounted_returns = []\n",
    "        \n",
    "    for timestep in range(len(rewards)):\n",
    "        discounted_return = calculate_discounted_return_time_step(rewards, timestep=timestep, discount_factor=discount_factor)\n",
    "        discounted_returns.append(discounted_return)\n",
    "    \n",
    "    return discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([14.4584]),\n",
       " tensor([14.6045]),\n",
       " tensor([13.7419]),\n",
       " tensor([11.8605]),\n",
       " tensor([8.9500]),\n",
       " tensor([5.])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_an_episode(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(rewards):\n",
    "    discount_factor = 0.99\n",
    "    discounted_rewards = []\n",
    "    n_rewards = len(rewards)\n",
    "\n",
    "\n",
    "    for t in range(n_rewards):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + discount_factor**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(14.4584),\n",
       " tensor(14.6045),\n",
       " tensor(13.7419),\n",
       " tensor(11.8605),\n",
       " tensor(8.9500),\n",
       " tensor(5.)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_policy(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
