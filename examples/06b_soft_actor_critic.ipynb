{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from octopus.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_observations, n_actions,\n",
    "        fc1_dim=256, fc2_dim=256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_observations[0] + n_actions, fc1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dim, fc2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        input = torch.cat([state, action], dim=1)\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, fc1_dim, fc2_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dim, fc2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.layers(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, max_action, fc1_dim=256, fc2_dim=256):\n",
    "        super().__init__()\n",
    "        self.reparam_noise = 1e-6\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dim, fc2_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(fc2_dim, n_actions)\n",
    "        self.sigma = nn.Linear(fc2_dim, n_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        prob = self.layers(state)\n",
    "        \n",
    "        # mean of distribution\n",
    "        mu = self.mu(state)\n",
    "        \n",
    "        # standard deviation of distribution\n",
    "        sigma = self.sigma(state)\n",
    "        sigma = torch.clamp(sigma, min=self.reparam_noise, max=1)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_normal(self, state, reparametrize=True):\n",
    "        mu, sigma = self.forward(state)\n",
    "        probs = Normal(mu, sigma)\n",
    "        \n",
    "        if reparametrize:\n",
    "            # do reparametrize trick\n",
    "            # add some noise to the acton!\n",
    "            actions = probs.rsample()\n",
    "        else:\n",
    "            actions = probs.sample()\n",
    "        \n",
    "        # scales the values of actions to the range [-self.max_action, self.max_action]\n",
    "        actions = torch.tanh(actions) * self.max_action\n",
    "        \n",
    "        log_probs = probs.log_prob(actions)\n",
    "        \n",
    "        # come from the paper's appendix\n",
    "        # c. Enforcing Action Bounds\n",
    "        log_probs -= torch.log(1 - actions.pow(2) + self.reparam_noise)\n",
    "        log_probs = log_probs.sum(1, keepdim=True)\n",
    "        \n",
    "        return actions, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, env, n_observations: int, n_actions: int, mem_size: int=1000000,\n",
    "        scale_entropy: float = 0.99, discount_factor: float=0.99, reward_scale=2,\n",
    "        batch_size: int=256,\n",
    "        fc1_dim: int = 256, fc2_dim: int = 256\n",
    "    ):\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.n_observations = n_observations\n",
    "        \n",
    "        self.scale_entropy = scale_entropy # tau\n",
    "        self.discount_factor = discount_factor # gamma\n",
    "        self.reward_scale = reward_scale\n",
    "        \n",
    "        self.memory = ReplayBuffer(mem_size, n_observations, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        max_action = env.action_space.high\n",
    "        \n",
    "        self.actor_network = ActorNetwork(\n",
    "            n_observations, n_actions, max_action,\n",
    "            fc1_dim, fc2_dim\n",
    "        )\n",
    "        \n",
    "        self.critic_1_network = CriticNetwork(n_observations, n_actions, fc1_dim, fc2_dim)\n",
    "        self.critic_2_network = CriticNetwork(n_observations, n_actions, fc1_dim, fc2_dim)\n",
    "    \n",
    "        self.value_network = ValueNetwork(n_observations, fc1_dim, fc2_dim)\n",
    "        self.target_value_network = ValueNetwork(n_observations, fc1_dim, fc2_dim)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor([state])\n",
    "        action, _ = self.actor.sample_normal(state, reparametrize=False)\n",
    "        action_idx = action[0]\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "        \n",
    "    def update_network_parameters(self, tau = None):\n",
    "        # at the begging of simulation, we want to set the value\n",
    "        # of the target network to exact copy of the value network\n",
    "        # but from there on, we want to be a soft copy\n",
    "        if tau is None:\n",
    "            tau = self.scale_entropy\n",
    "        \n",
    "        target_value_params = self.target_value_network.named_parameters()\n",
    "        value_params = self.value_network.named_parameters()\n",
    "        \n",
    "        target_value_state_dict = dict(target_value_params)\n",
    "        value_state_dict = dict(value_params)\n",
    "        \n",
    "        for name in value_state_dict:\n",
    "            scaled_target_value = (1-tau) * target_value_state_dict[name].clone()\n",
    "            value_state_dict[name] = tau*value_state_dict[name].clone() + scaled_target_value\n",
    "            \n",
    "        self.target_value_network.load_state_dict(value_state_dict)\n",
    "    \n",
    "    def store_transition(self): pass\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            # only learn if filled at least the batch size\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.float)\n",
    "        \n",
    "        value = self.value_network(state).view(dim=-1)\n",
    "        value_of_next_state = self.value_network(next_state).view(dim=-1)\n",
    "        value_of_next_state[done] = 0.0\n",
    "        \n",
    "        self.value_network.optimizer.zero_grad()\n",
    "        value_loss = self._get_value_loss(state)\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        self.value_network.optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: why do reparameterize\n",
    "    \n",
    "    def _get_critic_prediction(self, state: torch.Tensor, actions: torch.Tensor):\n",
    "        q1_new_policy = self.critic_1_network.forward(state, actions)\n",
    "        q2_new_policy = self.critic_2_network.forward(state, actions)\n",
    "        \n",
    "        critic_value = torch.min(q1_new_policy, q2_new_policy)\n",
    "        \n",
    "        return critic_value.view(dim=-1)\n",
    "    \n",
    "    def _get_value_loss(self, state):\n",
    "        actions, log_probs = self.actor_network.sample_normal(state, reparametrize=True)\n",
    "        log_probs = log_probs.view(dim=-1)\n",
    "        \n",
    "        pred_critic_value = self._get_critic_prediction(state, actions)\n",
    "        target_critic_value = pred_critic_value - log_probs\n",
    "        value_loss = 0.5 * F.mse_loss(pred_critic_value, target_critic_value)\n",
    "        \n",
    "        return value_loss\n",
    "    \n",
    "    def _get_actor_loss(self, state):\n",
    "        actions, log_probs = self.actor_network.sample_normal(state, reparametrize=True)\n",
    "        log_probs = log_probs.view(dim=-1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIC_LEARNING_RATE = 1e-3\n",
    "VALUE_LEARNING_RATE = 1e-3\n",
    "ACTOR_LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'n_observations' and 'n_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m critic_network \u001b[39m=\u001b[39m CriticNetwork()\n\u001b[1;32m      2\u001b[0m value_network \u001b[39m=\u001b[39m ValueNetwork()\n\u001b[1;32m      3\u001b[0m actor_network \u001b[39m=\u001b[39m ActorNetwork()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'n_observations' and 'n_actions'"
     ]
    }
   ],
   "source": [
    "critic_network = CriticNetwork()\n",
    "value_network = ValueNetwork()\n",
    "actor_network = ActorNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'critic_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m critic_optimizier \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(critic_network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mCRITIC_LEARNING_RATE)\n\u001b[1;32m      2\u001b[0m value_optimizier \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(value_network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mVALUE_LEARNING_RATE)\n\u001b[1;32m      3\u001b[0m actor_optimizier \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(actor_network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mACTOR_LEARNING_RATE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'critic_network' is not defined"
     ]
    }
   ],
   "source": [
    "critic_optimizier = optim.Adam(critic_network.parameters(), lr=CRITIC_LEARNING_RATE)\n",
    "value_optimizier = optim.Adam(value_network.parameters(), lr=VALUE_LEARNING_RATE)\n",
    "actor_optimizier = optim.Adam(actor_network.parameters(), lr=ACTOR_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://youtu.be/ioidsRlf79o?t=1230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
