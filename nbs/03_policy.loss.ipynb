{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp policy.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "from octopus.policy.reward import calculate_discounted_return_each_timestep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\gamma_t * G_t * \\log \\pi_s(a \\mid \\theta)$\n",
    "\n",
    "$G_t$: is called the sum of all rewards until time $t$\n",
    "- $G_t=r_t+r_{t+1} \\ldots+r_{T-1}+r_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(selected_actions, discounted_rewards):\n",
    "    return -1 * torch.sum(discounted_rewards * torch.log(selected_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reinforce_loss(log_probs: torch.Tensor, rewards: torch.Tensor, discount_factor: float) -> torch.Tensor:\n",
    "    \n",
    "    assert len(log_probs) == len(rewards)\n",
    "    \n",
    "    # selected_log_probs = selected_probs.log()\n",
    "    discounted_return_at_each_timestep = calculate_discounted_return_each_timestep(rewards, discount_factor)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for discounted_reward, log_prob in zip(discounted_return_at_each_timestep, log_probs):\n",
    "        total_loss += discounted_reward * -log_prob\n",
    "    \n",
    "    loss_mean = total_loss / len(log_probs)\n",
    "        \n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = [torch.tensor(0.1).log(), torch.tensor(0.2).log(), torch.tensor(0.3).log()]\n",
    "rewards = [torch.tensor(1), torch.tensor(2), torch.tensor(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4143)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_loss(log_probs, rewards, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
