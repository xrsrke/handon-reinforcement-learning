diff --git a/.gitignore b/.gitignore
index 4885cc8..934678e 100644
--- a/.gitignore
+++ b/.gitignore
@@ -163,4 +163,5 @@ cython_debug/
 runs
 octopus
 .vscode
-flashcards
\ No newline at end of file
+flashcards
+_proc
\ No newline at end of file
diff --git a/examples/05_policy_gradient.ipynb b/examples/05_policy_gradient.ipynb
index 4ee9ede..867b41a 100644
--- a/examples/05_policy_gradient.ipynb
+++ b/examples/05_policy_gradient.ipynb
@@ -164,7 +164,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.15"
+   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
   },
   "orig_nbformat": 4,
   "vscode": {
diff --git a/examples/06_actor_critic.ipynb b/examples/06_actor_critic.ipynb
index 075ecbd..406861e 100644
--- a/examples/06_actor_critic.ipynb
+++ b/examples/06_actor_critic.ipynb
@@ -2,13 +2,16 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
     "import torch\n",
     "from torch import nn\n",
-    "from torch import optim"
+    "from torch import optim\n",
+    "\n",
+    "import torch.nn.functional as F\n",
+    "import gym"
    ]
   },
   {
@@ -18,17 +21,76 @@
    "outputs": [],
    "source": [
     "class ActorCritic(nn.Module):\n",
-    "    def __init__(self, n_states):\n",
-    "        self.l1 = nn.Linear(n_states, 25)\n",
-    "        self.l2 = nn.Linear(25, 50)"
+    "    def __init__(self, n_observations, n_actions):\n",
+    "        # self.l1 = nn.Linear(n_states, 25)\n",
+    "        # self.l2 = nn.Linear(25, 50)\n",
+    "        # self.actor_lin1 = nn.Linear(50, 2)\n",
+    "        # self.l3 = nn.Linear(50, 25)\n",
+    "        # self.critic_lin1 = nn.Linear(25, 1)\n",
+    "        \n",
+    "        self.actor = nn.Sequential(\n",
+    "            nn.Linear(n_observations, 25),\n",
+    "            nn.ReLU(),\n",
+    "            nn.Linear(25, n_actions),\n",
+    "            nn.ReLU(),\n",
+    "            nn.LogSoftmax(dim=-1)\n",
+    "        )\n",
+    "        \n",
+    "        self.critic = nn.Sequential(\n",
+    "            nn.Linear(n_observations, 25),\n",
+    "            nn.ReLU(),\n",
+    "            nn.Linear(25, n_actions),\n",
+    "            nn.ReLU(),\n",
+    "            nn.Linear(50,25),\n",
+    "            nn.ReLU(),\n",
+    "            nn.Tanh()\n",
+    "        )\n",
+    "    \n",
+    "    def forward(self, x):\n",
+    "        x = F.normalize(x, dim=0)\n",
+    "        actor = self.actor(x)\n",
+    "        critic = self.critic(x)\n",
+    "        \n",
+    "        return actor, critic"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": []
+  },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
-   "source": []
+   "source": [
+    "def run_episode(worker_env, worker_model):\n",
+    "    state = torch.from_numpy(worker_env.env.state).float()\n",
+    "    values, log_probs, rewards = [], [], []\n",
+    "    \n",
+    "    done = False\n",
+    "    j = 0\n",
+    "    while (done == False):\n",
+    "        j += 1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def worker(t, worker_model, counter, params):\n",
+    "    worker_env = gym.make(\"CartPole-v1\")\n",
+    "    worker_env.reset()\n",
+    "    worker_optim = optim.Adam(worker_model.parameters(), lr=1e-4)\n",
+    "    worker_optim.zero_grad()\n",
+    "    \n",
+    "    for i in range(params[\"epochs\"]):\n",
+    "        worker_optim.zero_grad()\n",
+    "        values, log_probs, rewards = run_episode(worker_env, worker_model)"
+   ]
   }
  ],
  "metadata": {
@@ -47,7 +109,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.8.15"
+   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
   },
   "orig_nbformat": 4,
   "vscode": {
diff --git a/nbs/03_policy.reward.ipynb b/nbs/03_policy.reward.ipynb
index a5be2d0..13abf6f 100644
--- a/nbs/03_policy.reward.ipynb
+++ b/nbs/03_policy.reward.ipynb
@@ -13,11 +13,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| default_exp policy.reward"
@@ -26,11 +22,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| hide\n",
@@ -40,11 +32,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| hide\n",
@@ -53,15 +41,13 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "execution_count": 3,
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
+    "from typing import List\n",
+    "\n",
     "import torch"
    ]
   },
@@ -76,11 +62,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
@@ -101,11 +83,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
@@ -122,11 +100,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [
     {
      "data": {
@@ -154,11 +128,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
@@ -174,11 +144,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
@@ -189,11 +155,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [
     {
      "data": {
@@ -213,11 +175,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
@@ -239,11 +197,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "rewards = [torch.tensor(5), torch.tensor(2), torch.tensor(3), torch.tensor(4)]"
@@ -252,11 +206,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [
     {
      "data": {
@@ -276,11 +226,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "#| export\n",
@@ -298,11 +244,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
@@ -311,11 +253,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [
     {
      "data": {
@@ -340,11 +278,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "def update_policy(rewards):\n",
@@ -367,11 +301,7 @@
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
-    }
-   },
+   "metadata": {},
    "outputs": [
     {
      "data": {
@@ -393,14 +323,67 @@
     "update_policy(rewards)"
    ]
   },
+  {
+   "attachments": {},
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Advantage Function"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "python"
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#| export\n",
+    "def calculate_advantages(discounted_returns: torch.Tensor, q_values: torch.Tensor) -> torch.Tensor:\n",
+    "    assert len(discounted_returns) == len(q_values)\n",
+    "    return discounted_returns - q_values"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "discounted_returns = torch.tensor([10, 20, 30, 40])\n",
+    "q_values = torch.tensor([1, 2, 3, 4])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "tensor([ 9, 18, 27, 36])"
+      ]
+     },
+     "execution_count": 15,
+     "metadata": {},
+     "output_type": "execute_result"
     }
-   },
+   ],
+   "source": [
+    "calculate_advantages(discounted_returns, q_values)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
    "outputs": [],
    "source": []
   }
@@ -410,6 +393,23 @@
    "display_name": "gym",
    "language": "python",
    "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.8.15"
+  },
+  "vscode": {
+   "interpreter": {
+    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
+   }
   }
  },
  "nbformat": 4,
diff --git a/requirements.txt b/requirements.txt
index 4e6d795..d223900 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,4 +6,5 @@ huggingface-hub
 huggingface-sb3
 nbdev
 moviepy
-gymnasium
\ No newline at end of file
+gymnasium
+wandb
\ No newline at end of file
diff --git a/tests/policy/test_reward.py b/tests/policy/test_reward.py
index 02c02b4..d34be42 100644
--- a/tests/policy/test_reward.py
+++ b/tests/policy/test_reward.py
@@ -1,7 +1,11 @@
 import torch
+# from torch.testing import assertEqual
 import pytest
 
-from octopus.policy.reward import calculate_discounted_return_each_timestep
+from octopus.policy.reward import (
+    calculate_discounted_return_each_timestep,
+    calculate_advantages
+)
 
 @pytest.mark.parametrize(
     "rewards, discount_factor, expected_output",
@@ -20,4 +24,21 @@ from octopus.policy.reward import calculate_discounted_return_each_timestep
 )
 def test_calculate_discounted_return_each_timestep(rewards, discount_factor, expected_output):
     output = calculate_discounted_return_each_timestep(rewards, discount_factor)
-    torch.testing.assert_close(output, expected_output)
\ No newline at end of file
+    torch.testing.assert_close(output, expected_output)
+
+
+def test_calculate_advantages():
+    discounted_returns = torch.tensor([10, 20, 30, 40])
+    q_values = torch.tensor([1, 2, 3, 4])
+    expected_advantages = torch.tensor([9, 18, 27, 36])
+
+    advantages = calculate_advantages(discounted_returns, q_values)
+
+    assert (advantages == expected_advantages).all()
+
+def test_calculate_advantages_raise_non_equal_length():
+    discounted_returns = torch.tensor([10, 20, 30, 40])
+    q_values = torch.tensor([1, 2])
+
+    with pytest.raises(AssertionError):
+        calculate_advantages(discounted_returns, q_values)
\ No newline at end of file
