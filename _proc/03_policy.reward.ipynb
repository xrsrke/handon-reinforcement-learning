{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fill in a module description here\n",
    "output-file: policy.reward.html\n",
    "title: Rewards\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### discount_rewards\n",
       "\n",
       ">      discount_rewards (rewards, discount_factor)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### discount_rewards\n",
       "\n",
       ">      discount_rewards (rewards, discount_factor)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(discount_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the discounted return of a timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L25){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_time_step\n",
       "\n",
       ">      calculate_discounted_return_time_step (rewards, timestep,\n",
       ">                                             discount_factor)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L25){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_time_step\n",
       "\n",
       ">      calculate_discounted_return_time_step (rewards, timestep,\n",
       ">                                             discount_factor)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(calculate_discounted_return_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_hello\n",
       "\n",
       ">      test_hello ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### test_hello\n",
       "\n",
       ">      test_hello ()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(test_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.7419])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_time_step(rewards, timestep=2, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_each_timestep\n",
       "\n",
       ">      calculate_discounted_return_each_timestep (rewards, discount_factor)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_each_timestep\n",
       "\n",
       ">      calculate_discounted_return_each_timestep (rewards, discount_factor)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(calculate_discounted_return_each_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "rewards = [torch.tensor(5), torch.tensor(2), torch.tensor(3), torch.tensor(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.8015,  8.8904,  6.9600,  4.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_each_timestep(rewards, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_an_episode\n",
       "\n",
       ">      calculate_discounted_return_an_episode (rewards, discount_factor=0.99)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/reinforcement-learning/blob/main/octopus/policy/reward.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### calculate_discounted_return_an_episode\n",
       "\n",
       ">      calculate_discounted_return_an_episode (rewards, discount_factor=0.99)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(calculate_discounted_return_an_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "rewards = torch.tensor([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([14.4584]),\n",
       " tensor([14.6045]),\n",
       " tensor([13.7419]),\n",
       " tensor([11.8605]),\n",
       " tensor([8.9500]),\n",
       " tensor([5.])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_discounted_return_an_episode(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def update_policy(rewards):\n",
    "    discount_factor = 0.99\n",
    "    discounted_rewards = []\n",
    "    n_rewards = len(rewards)\n",
    "\n",
    "\n",
    "    for t in range(n_rewards):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + discount_factor**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(14.4584),\n",
       " tensor(14.6045),\n",
       " tensor(13.7419),\n",
       " tensor(11.8605),\n",
       " tensor(8.9500),\n",
       " tensor(5.)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_policy(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
