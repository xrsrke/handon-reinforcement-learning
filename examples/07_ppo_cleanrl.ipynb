{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # \"exp_name\": os.path.basename(__file__).rstrip(\".py\"),\n",
    "    \"exp_name\": \"clean\",\n",
    "    \"seed\": 1,\n",
    "    \"torch_deterministic\": True,\n",
    "    \"cuda\": True,\n",
    "    \"track\": False,\n",
    "    \"wandb_project_name\": \"cleanRL\",\n",
    "    \"wandb_entity\": None,\n",
    "    \"capture_video\": False,\n",
    "    \"env_id\": \"CartPole-v1\",\n",
    "    \"total_timesteps\": 500000,\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"num_envs\": 4,\n",
    "    \"num_steps\": 128,\n",
    "    \"anneal_lr\": True,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"num_minibatches\": 4,\n",
    "    \"update_epochs\": 4,\n",
    "    \"norm_adv\": True,\n",
    "    \"clip_coef\": 0.2,\n",
    "    \"clip_vloss\": True,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"use_tfboard\": True,\n",
    "    \"lr_schedule\": \"linear\",\n",
    "    \"debug\": False,\n",
    "    \"lr_warmup\": 0,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": 4,\n",
    "    \"sde_support_size\": 100,\n",
    "    \"sde_num_atoms\": 50,\n",
    "    \"sde_scaling\": 0.5,\n",
    "    \"use_rnd\": False,\n",
    "    \"rnd_normalize_reward\": False,\n",
    "    \"rnd_normalize_observation\": False,\n",
    "    \"rnd_ignore_done\": False,\n",
    "    \"no_tensorboard\": False,\n",
    "    \"no_wandb\": False,\n",
    "}\n",
    "\n",
    "params[\"batch_size\"] = int(params[\"num_envs\"] * params[\"num_steps\"])\n",
    "params[\"minibatch_size\"] = int(params[\"batch_size\"] // params[\"num_minibatches\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "\n",
    "        # if not seed:\n",
    "        #     seed = 200\n",
    "\n",
    "        # env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Agent(nn.Module):\n",
    "#     def __init__(self, n_observations, n_actions):\n",
    "#         super().__init__()\n",
    "#         self.critic = nn.Sequential(\n",
    "#             layer_init(nn.Linear(n_observations, 64)),\n",
    "#             nn.Tanh(),\n",
    "#             layer_init(nn.Linear(64, 64)),\n",
    "#             nn.Tanh(),\n",
    "#             layer_init(nn.Linear(64, 1), std=1.0),\n",
    "#         )\n",
    "#         self.actor = nn.Sequential(\n",
    "#             layer_init(nn.Linear(n_observations, 64)),\n",
    "#             nn.Tanh(),\n",
    "#             layer_init(nn.Linear(64, 64)),\n",
    "#             nn.Tanh(),\n",
    "#             layer_init(nn.Linear(64, n_actions), std=0.01),\n",
    "#         )\n",
    "\n",
    "#     def get_value(self, x):\n",
    "#         return self.critic(x)\n",
    "\n",
    "#     def get_action_and_value(self, x, action=None):\n",
    "#         logits = self.actor(x)\n",
    "#         probs = Categorical(logits=logits)\n",
    "#         if action is None:\n",
    "#             action = probs.sample()\n",
    "#         return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(env_id)\n",
    "# n_observations = env.observation_space.shape[0]\n",
    "# n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CartPole-v1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.env_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    # [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    [make_env(args.env_id, random.randint(3, 1000), i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(n_observations, n_actions)\n",
    "#optimizer = optim.Adam(agent.parameters(), lr=1e-3, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "_state, _ = envs.reset()\n",
    "next_obs = torch.Tensor(_state).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "num_updates = args.total_timesteps // args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m logprobs[step] \u001b[39m=\u001b[39m log_prob\n\u001b[1;32m     18\u001b[0m \u001b[39m# next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# rewards[step] = torch.tensor(reward).to(device).view(-1)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# next_obs, next_done = torch.tensor(next_obs).to(device), torch.tensor(done).to(device)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m next_obs, reward, done, _, info \u001b[39m=\u001b[39m envs\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     23\u001b[0m rewards[step] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(reward)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m next_obs, next_done \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(next_obs)\u001b[39m.\u001b[39mto(device), torch\u001b[39m.\u001b[39mTensor(done)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/vector/vector_env.py:137\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m    Batch of (observations, rewards, terminated, truncated, infos) or (observations, rewards, dones, infos)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/vector/sync_vector_env.py:159\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m     observations\u001b[39m.\u001b[39mappend(observation)\n\u001b[1;32m    158\u001b[0m     infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_info(infos, info, i)\n\u001b[0;32m--> 159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations \u001b[39m=\u001b[39m concatenate(\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msingle_observation_space, observations, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservations\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    164\u001b[0m     deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations,\n\u001b[1;32m    165\u001b[0m     np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rewards),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     infos,\n\u001b[1;32m    169\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/functools.py:875\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    872\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    873\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 875\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/gym/vector/utils/numpy_utils.py:50\u001b[0m, in \u001b[0;36m_concatenate_base\u001b[0;34m(space, items, out)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m@concatenate\u001b[39m\u001b[39m.\u001b[39mregister(Box)\n\u001b[1;32m     46\u001b[0m \u001b[39m@concatenate\u001b[39m\u001b[39m.\u001b[39mregister(Discrete)\n\u001b[1;32m     47\u001b[0m \u001b[39m@concatenate\u001b[39m\u001b[39m.\u001b[39mregister(MultiDiscrete)\n\u001b[1;32m     48\u001b[0m \u001b[39m@concatenate\u001b[39m\u001b[39m.\u001b[39mregister(MultiBinary)\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_concatenate_base\u001b[39m(space, items, out):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mstack(items, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/numpy/core/shape_base.py:433\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    431\u001b[0m sl \u001b[39m=\u001b[39m (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m),) \u001b[39m*\u001b[39m axis \u001b[39m+\u001b[39m (_nx\u001b[39m.\u001b[39mnewaxis,)\n\u001b[1;32m    432\u001b[0m expanded_arrays \u001b[39m=\u001b[39m [arr[sl] \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(expanded_arrays, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for update in range(1, num_updates +1):\n",
    "    \n",
    "    # anneal learning rate schedule\n",
    "    \n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += 1 * args.num_envs\n",
    "        \n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy, critic_value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = critic_value.flatten()\n",
    "        \n",
    "        actions[step] = action\n",
    "        logprobs[step] = log_prob\n",
    "        \n",
    "        # next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy())\n",
    "        # rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        # next_obs, next_done = torch.tensor(next_obs).to(device), torch.tensor(done).to(device)\n",
    "        \n",
    "        next_obs, reward, done, _, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "    \n",
    "    # TODO: understand how to calcualte estimate advantage\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "        \n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    \n",
    "    # optimize the policy and value network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
