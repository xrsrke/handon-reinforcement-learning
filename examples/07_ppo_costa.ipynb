{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym_id = \"CartPole-v1\"\n",
    "# LEARNING_RATE = 1e-3\n",
    "# N_ENVS = 4\n",
    "# N_STEPS = 1000\n",
    "\n",
    "# # N_UPDATES = 10\n",
    "# BATCH_SIZE = N_ENVS * N_STEPS\n",
    "\n",
    "# EXP_NAME = \"TESTING\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"GYM_ID\": \"CartPole-v1\",\n",
    "    \"LEARNING_RATE\": 1e-3,\n",
    "    \"N_ENVS\": 4,\n",
    "    \"N_STEPS\": 1000,\n",
    "    \"EXP_NAME\": \"TESTING\",\n",
    "    \"GAE\": True,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95\n",
    "}\n",
    "\n",
    "PARAMS[\"BATCH_SIZE\"] = PARAMS[\"N_ENVS\"] * PARAMS[\"N_STEPS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CartPole-v1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F\"{PARAMS['GYM_ID']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{PARAMS['GYM_ID']}_{PARAMS['EXP_NAME']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{PARAMS['EXP_NAME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gym_id):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        # env = gym.wrappers.RecordVideo(env, \"videos\", episode_trigger=)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.SyncVectorEnv([make_env(gym_id)])\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(PARAMS['GYM_ID']) for i in range(PARAMS['N_ENVS'])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m observation, reward, done, truncated, info \u001b[39m=\u001b[39m envs \u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      6\u001b[0m \u001b[39m# episodic_return += reward\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m     observation, _ \u001b[39m=\u001b[39m envs\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     10\u001b[0m     episodic_return \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "observation, _ = envs.reset()\n",
    "# episodic_return = 0\n",
    "for _ in range(200):\n",
    "    action = envs.action_space.sample()\n",
    "    observation, reward, done, truncated, info = envs .step(action)\n",
    "    # episodic_return += reward\n",
    "     \n",
    "    if done:\n",
    "        observation, _ = envs.reset()\n",
    "        episodic_return = 0\n",
    "        \n",
    "        print(\"episodic_return = \", info['episode']['r'])\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        prod_observations = np.array(envs.single_observation_space.shape).prod()\n",
    "        n_actions = envs.single_action_space.n\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(prod_observations, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(prod_observations, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, n_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        return self.critic(state)\n",
    "    \n",
    "    def get_action_and_value(self, state, action=None):\n",
    "        logits = self.actor(state)\n",
    "        \n",
    "        probs = Categorical(logits=logits)\n",
    "        \n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (actor): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(agent.parameters(), lr=PARAMS['LEARNING_RATE'], eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = envs.reset()\n",
    "state = torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0019],\n",
       "        [-0.0194],\n",
       "        [ 0.0121],\n",
       "        [-0.0157]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_value(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(PARAMS['N_STEPS'], PARAMS['N_ENVS']) + envs.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape(obs) = [N_STEPS, N_ENV, OBS]\n",
    "obs = torch.zeros((PARAMS['N_STEPS'], PARAMS['N_ENVS']) + envs.single_observation_space.shape)\n",
    "actions = torch.zeros((PARAMS['N_STEPS'], PARAMS['N_ENVS']) + envs.single_action_space.shape)\n",
    "log_probs = torch.zeros((PARAMS['N_STEPS'], PARAMS['N_ENVS']))\n",
    "rewards = torch.zeros((PARAMS['N_STEPS'], PARAMS['N_ENVS']))\n",
    "dones = torch.zeros((PARAMS['N_STEPS'], PARAMS['N_ENVS']))\n",
    "values = torch.zeros((PARAMS['N_ENVS'], PARAMS['N_ENVS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset()\n",
    "next_obs = torch.tensor(next_obs)\n",
    "next_done = torch.tensor(PARAMS['N_ENVS'])\n",
    "\n",
    "N_UPDATES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m next_obs, next_done \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(next_obs), torch\u001b[39m.\u001b[39mtensor(done)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m info:\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m item\u001b[39m.\u001b[39;49mkeys():\n\u001b[1;32m     20\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mglobal_step=\u001b[39m\u001b[39m{\u001b[39;00mglobal_step\u001b[39m}\u001b[39;00m\u001b[39m, episodic_return=\u001b[39m\u001b[39m{\u001b[39;00mitem[\u001b[39m'\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mcharts/episodic_return\u001b[39m\u001b[39m\"\u001b[39m, item[\u001b[39m\"\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m], global_step)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "for update in range(1, N_UPDATES + 1):\n",
    "    for step in range(0, PARAMS['N_ENVS']):\n",
    "        global_step += 1 * PARAMS['N_ENVS']\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs) \n",
    "            values[step] = value.flatten()\n",
    "        \n",
    "        actions[step] = action\n",
    "        log_probs[step] = log_prob\n",
    "        \n",
    "        next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).view(-1)\n",
    "        next_obs, next_done = torch.tensor(next_obs), torch.tensor(done)\n",
    "                \n",
    "        for item in info:\n",
    "            if \"episode\" in item.keys():\n",
    "                print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                break\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
