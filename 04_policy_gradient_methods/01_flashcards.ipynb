{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1003bed7-86f4-4c73-aeee-e452b681c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170334ec-6e8e-4932-b67c-8f5f88331617",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535d6ce6-4899-40e0-a0aa-6486b6c1df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98c02128-319e-4325-bd34-44b951391c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bc306-b8ae-4df7-a9f6-26bffaf1893b",
   "metadata": {},
   "source": [
    "Given `rewards` is the list of all predicted rewards from the current time step `0` to the next time step `3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be86cd4c-f505-43fa-92ea-8b9b448ae291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255d0ca-e7e6-437f-b23e-19dd6362110c",
   "metadata": {},
   "source": [
    "Write a function to calcualte the discounted reward at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6af7de7-db88-4e12-8343-6a018adb3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15e8f0b6-abcf-4e61-abcd-c23e16cde6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, discount_factor):\n",
    "    n_rewards = len(rewards)\n",
    "    timesteps = torch.arange(0, n_rewards)\n",
    "    \n",
    "    # calculate the discount for each time step\n",
    "    discount = torch.pow(discount_factor, timesteps)\n",
    "    discounted_rewards = discount * rewards\n",
    "\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bd486f3-4d62-47d7-a91c-420a80a8aedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.9800, 2.9403, 3.8812])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_reward(rewards, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580d1f2-1abd-409b-aebd-9afac66de07f",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c7a2a65-09f1-4f49-971a-2eb0bf127bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor([[0.1, 0.3, 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "335963b0-ff2b-4b7b-a3ca-35e1d5efef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f46ab-2bb2-4a8a-8e68-13cd7a208bb1",
   "metadata": {},
   "source": [
    "`preds` is the probability distribution over the possible actions at the current time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2550a517-2b9c-4f1e-a40f-de20e5018100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.3000, 0.6000]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc529f0-f6c0-449e-ac4f-e0a40fd87cc5",
   "metadata": {},
   "source": [
    "Sample an action from `preds` using PyTorch's built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37f28033-4248-41e3-8fd2-d5a1e7e27e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.multinomial(preds, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "694b8d26-234a-4f82-8235-7933d2e50b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180ef88-35bc-4980-a15a-b3b20bbb10ac",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeeb26a3-45c8-4193-bdbf-ea1114df4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor([[0.1, 0.3, 0.6], [0.7, 0.2, 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9b3721-b1b0-4f8d-a2c1-82c0bda43b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_reward = torch.tensor([1.0, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a2630-43f3-4201-9344-029ce846cc77",
   "metadata": {},
   "source": [
    "- `preds` is the probability distribution over the possible actions at each time step\n",
    "- `discounted_reward` is the distribution of discounted rewards at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea6263e-08ef-4e52-a691-98b9fc5a5bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.3000, 0.6000],\n",
       "        [0.7000, 0.2000, 0.1000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179ad8cb-257c-4dda-b158-1da7fe552c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.5000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0588f6-e53c-4697-9ae8-09d219be1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(preds, discounted_rewards):\n",
    "    return -1 * torch.sum(discounted_rewards * torch.log(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30025c37-307c-4c8c-af1e-c1706aa963bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscounted_reward\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mloss_func\u001b[0;34m(preds, discounted_rewards)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_func\u001b[39m(preds, discounted_rewards):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mdiscounted_rewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "loss_func(preds, discounted_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bac6f9-bf3b-4e77-b122-bb4c55910a4e",
   "metadata": {},
   "source": [
    "### Transitions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b0e4c-1c0a-4ae7-ac22-01b29030e737",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ceb7d35-c56d-498e-ae3c-3490368d2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]\n",
    "hidden_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d8c188-c655-41d4-9ad6-6065c124e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n_observations, hidden_size),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(hidden_size, n_actions),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b642df6-6f4f-4a9c-9c93-c546463ca6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17381b6-cdc3-4113-8bf5-4340e575ea6f",
   "metadata": {},
   "source": [
    "Write a function generate transitions in one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a2f0c81-e588-48c6-aae0-c9f6545cebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transitions(model, env):\n",
    "    transitions = []\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        predicted_action = model(torch.from_numpy(state))\n",
    "        action = torch.argmax(predicted_action, dim=-1)\n",
    "        next_state, reward, done, truncated, info = env.step(action.item())\n",
    "        \n",
    "        transitions.append((\n",
    "            state, action, reward, next_state\n",
    "        ))\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9e6a911-d9a8-489a-ac35-679c4bd3cf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.modules.container.Sequential, gym.wrappers.time_limit.TimeLimit)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model), type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "542d930f-7326-451b-9f5a-45c3ac9fd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = generate_transitions(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9137bcc-14bd-424f-822a-bb160a30aef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-0.04570273, -0.00080109,  0.03115952,  0.04477873], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.04571875, -0.19635567,  0.0320551 ,  0.34712765], dtype=float32)),\n",
       " (array([-0.04571875, -0.19635567,  0.0320551 ,  0.34712765], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.04964587, -0.39191857,  0.03899765,  0.64974385], dtype=float32)),\n",
       " (array([-0.04964587, -0.39191857,  0.03899765,  0.64974385], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.05748424, -0.5875614 ,  0.05199253,  0.9544474 ], dtype=float32)),\n",
       " (array([-0.05748424, -0.5875614 ,  0.05199253,  0.9544474 ], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.06923547, -0.7833428 ,  0.07108147,  1.2630016 ], dtype=float32)),\n",
       " (array([-0.06923547, -0.7833428 ,  0.07108147,  1.2630016 ], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.08490232, -0.9792979 ,  0.09634151,  1.5770723 ], dtype=float32)),\n",
       " (array([-0.08490232, -0.9792979 ,  0.09634151,  1.5770723 ], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.10448828, -1.1754267 ,  0.12788296,  1.898182  ], dtype=float32)),\n",
       " (array([-0.10448828, -1.1754267 ,  0.12788296,  1.898182  ], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.12799682, -1.371681  ,  0.16584659,  2.2276545 ], dtype=float32)),\n",
       " (array([-0.12799682, -1.371681  ,  0.16584659,  2.2276545 ], dtype=float32),\n",
       "  tensor(0),\n",
       "  1.0,\n",
       "  array([-0.15543044, -1.5679474 ,  0.21039969,  2.5665505 ], dtype=float32))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e30f4b-2d4a-4307-80f4-41eff96f3cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfcbfb-bdd2-42de-86a8-4bfc8565a641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
