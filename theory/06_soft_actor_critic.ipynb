{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d390ee4-c36d-468c-abb8-f31d3c692683",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b97434-5494-45c1-9135-e252c414b65b",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd101463-e6c1-4b2a-b2c3-dcbb678ca0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6474ae4-680e-4117-87ed-b6fc94a0054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "std = torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ee15e7-17d2-43db-8b7f-1535061f8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Normal(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c85236-a158-444d-9904-c1e6d7691665",
   "metadata": {},
   "source": [
    "Sample an action from the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4d4668-c719-43ca-88a3-7c23db1df23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebfeda10-0fc9-4f06-b833-0b1e850324cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6733, -0.2301,  1.6005])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7eb64-6d81-4e47-ac56-e4ebe0d382cb",
   "metadata": {},
   "source": [
    "Calculate the log probability of the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7dae2b-c86f-487b-9138-8f6031b3796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa50bc04-23c3-40a8-8e12-fdd345528d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1456, -0.9454, -2.1998])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac9e1b",
   "metadata": {},
   "source": [
    "### 3.2 Maximum Entropy Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdba59",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d927b",
   "metadata": {},
   "source": [
    "$J(\\pi)=\\sum_{t=0}^T \\mathbb{E}_{\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right) \\sim \\rho_\\pi}\\left[r\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)+\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot \\mid \\mathbf{s}_t\\right)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331a8ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1caf9277-0c01-44e5-944a-9047ddaf8a92",
   "metadata": {},
   "source": [
    "# 1. Soft Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d43bae-e94e-4a6c-af2d-46eeabc4b212",
   "metadata": {},
   "source": [
    "### 1.1 Policy evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78996731-51af-4ecd-ad5b-39753ff418d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f486c-c11c-49d3-9d4f-20cb69ce778e",
   "metadata": {},
   "source": [
    "Write a pseudo-code to update the Q function. According to the policy evaluation step's equation:\n",
    "\n",
    "$\\mathcal{T}^\\pi Q\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right) \\triangleq r\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p}\\left[V\\left(\\mathbf{s}_{t+1}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80821530-296d-44e4-be74-cd3e0d907264",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "- `n_steps` is the number of time steps that an agent interacts in an episode\n",
    "- `states` and `actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae278e7-a8d6-4779-8920-2d296928336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_steps):\n",
    "    # Compute the reward for the current time step\n",
    "    reward = compute_reward(states[t], actions[t])\n",
    "    \n",
    "    # Compute the expected value of the next state\n",
    "    expected_value = value_function(states[t+1]).mean()\n",
    "    \n",
    "    # Update the value function using the modified Bellman backup operator\n",
    "    Q[t] = reward + gamma * expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b3dff-dc43-446f-8d1d-8ff61b66051d",
   "metadata": {},
   "source": [
    "### 1.2 Policy improvement step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465fb91-0112-4883-8a4d-c082cc6df94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a0ddb-701f-4679-a5c2-7e169954d6b8",
   "metadata": {},
   "source": [
    "$\\pi_{\\text {new }}=\\arg \\min _{\\pi^{\\prime} \\in \\Pi} D_{K L}\\left(\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{s}_t\\right) \\| \\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d752ca5-bc17-47b6-8e31-16fb017568b6",
   "metadata": {},
   "source": [
    "In the context of soft actor-critic. Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbef50-00b5-4bdf-b1b5-ce13b736b6c8",
   "metadata": {},
   "source": [
    "Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c2694-81d1-4477-aba7-a4cae11baf3a",
   "metadata": {},
   "source": [
    "**1. What does the equation do?**\n",
    "\n",
    "**Answer**: Minimize the difference between two probability distributions: $\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{S}_t\\right)$ and $\\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}\\left(\\mathbf{s}_t\\right)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236b98b-2671-4271-8687-d9581a92552b",
   "metadata": {},
   "source": [
    "`Notations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b67934-de8e-45ea-bc1c-0566b668555d",
   "metadata": {},
   "source": [
    "**2. What does $\\pi^{\\prime} \\in \\Pi$ mean?**\n",
    "\n",
    "**Answer**: Restricts the possible new policies to a set of policies \n",
    "\n",
    "\n",
    "**3. What does $Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)$ mean? Why need it?**\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- A scalar value that is used to normalize the distribution represented by the exponentiated Q-function\n",
    "- Ensures that the distribution sums to 1 over all actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067c96e-dbb7-40d7-84de-53543b8fddc2",
   "metadata": {},
   "source": [
    "`Q-Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac987b18-736a-472f-95a8-6d9b7b394625",
   "metadata": {},
   "source": [
    "**4. What does $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ mean?**\n",
    "\n",
    "**Answer**: It's a function of actions. So it takes in an action as an input and produces a scalar output.\n",
    "\n",
    "\n",
    "\n",
    "**5. What does $\\cdot$ $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ represent?**\n",
    "\n",
    "**Answer**: The dot indicates that the action component of the state-action pair has not been specified yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c1735-e9ae-48e6-ac2c-ba9ee2164644",
   "metadata": {},
   "source": [
    "**6. Why is the old $\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t,{\\cdot}\\right)\\right)$ the target for the new policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc66ea-504f-4080-b3a8-ef2f8b2b9ad0",
   "metadata": {},
   "source": [
    "**7. Why is the exponential of $Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)$ taken in this equation?**\n",
    "\n",
    "**Answer**: Ensure that the resulting distribution is a valid probability distribution, since the exponential function maps all real numbers to positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c73a4-937e-4dd2-9ea3-cdb5bc3c2376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c802c87-61f1-40a8-8384-66df905527ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32740f44-968a-4921-91e2-19cf8995bbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59205cd-5278-49d2-8a63-deeae393ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92fcad-6539-4d0b-a42e-de7f576a3480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b06bca3-3f57-4d33-acf0-3a45767d4067",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f6806-e724-43f6-9672-e1389dbb5460",
   "metadata": {
    "tags": []
   },
   "source": [
    "Policy improvement step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6cf15-775d-4f6a-9df5-ee0ea4a8e718",
   "metadata": {},
   "source": [
    "$\\pi_{\\text {new }}=\\arg \\min _{\\pi^{\\prime} \\in \\Pi} D_{K L}\\left(\\pi^{\\prime}\\left(\\cdot \\mid \\mathbf{s}_t\\right) \\| \\frac{\\exp \\left(Q^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t, \\cdot\\right)\\right)}{Z^{\\pi_{\\text {old }}}\\left(\\mathbf{s}_t\\right)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c435ab-8200-4c5d-88e6-e453ccad2ad6",
   "metadata": {},
   "source": [
    "Wha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3fb01",
   "metadata": {},
   "source": [
    "### Enforcing Action Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83255c16",
   "metadata": {},
   "source": [
    "$\\mu(\\mathbf{u} \\mid \\mathbf{s})$ is the density of the unbounded Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55704e9",
   "metadata": {},
   "source": [
    "$\\pi(\\tanh (\\mathbf{u}) \\mid \\mathbf{s})$ is the density of the bounded action distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5283df",
   "metadata": {},
   "source": [
    "Ensures that the new bounded action distribution still has the same meaning or interpretation as the old unbounded Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5979224",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "- $\\pi$ and $\\mu$: just a convention for the notation of the new and the old distribution. They're the same meaning\n",
    "- $\\mathrm{d} \\mathbf{a} / \\mathrm{d} \\mathbf{u}=\\operatorname{diag}\\left(1-\\tanh ^2(\\mathbf{u})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cdf27",
   "metadata": {},
   "source": [
    "$\\log \\pi(\\tanh (\\mathbf{u}) \\mid \\mathbf{s})=\\log \\mu(\\mathbf{u} \\mid \\mathbf{s})-\\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f83485",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "##### Question 1: Explain why use $\\tanh (\\mathbf{u})$\n",
    "\n",
    "**Explain**\n",
    "\n",
    "Because the action $u$ is being taken from a normal distribution with infinite support. But in practice, the action must be bounded to a finite interval.\n",
    "\n",
    "To enforce this bound, simply applies squashing function.\n",
    "\n",
    "#### Question 3: What is the math behind this equation?\n",
    "\n",
    "Use the Jacobian of a transformation to transform \n",
    "\n",
    "#### Question 2: Why minus $\\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$?\n",
    "\n",
    "**Explain**\n",
    "\n",
    "To transform the unbounded $\\mu(\\mathbf{u} \\mid \\mathbf{s})$ to bounded $\\pi(\\tanh (\\mathbf{u}) \\mid \\mathbf{s})$ but still has the same meaning as the old one. \n",
    "\n",
    "We apply transformation according to: $f_Y(y)=f_X(x)\\left|\\frac{d x}{d y}\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca1083-7540-43f4-818b-c6698f237456",
   "metadata": {},
   "source": [
    "### Loss Function of soft value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13da17b-a3d2-4776-983a-3171ec97f15e",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "959e53a6-1301-41cb-8bb0-cd1906531ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f876931-8c09-4d7b-ac1c-5713b79fd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_function = torch.nn.Linear(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d0faa4-71f7-4453-be19-5b1150e4f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_function = torch.nn.Linear(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f498e98-9d14-4920-94c8-10ab0ed6a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs = torch.randn(10, 6)\n",
    "action_probs = F.softmax(action_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a5ccb1-5626-46cc-9bfe-0762e12e277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.randn(10, 5)\n",
    "actions = torch.randn(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6672521-cf70-4b34-99f3-08bb670f5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(s, a):\n",
    "    # Define your q function here, for example:\n",
    "    q_prediction = torch.sum(s * a)\n",
    "    return q_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3daf8dee-1d7f-421e-8132-b956be468ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.q_function(s, a)>,\n",
       " Linear(in_features=5, out_features=3, bias=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_function, v_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747fe3a-4696-498d-8329-ee749b676df1",
   "metadata": {},
   "source": [
    "Given\n",
    "- `action_probs`: a probability distribution over all possible actions at each time step\n",
    "- `q_function`, `v_function`, `states`, `actions`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7159e8b-0efb-4c49-b566-79a445943a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 5]), torch.Size([10, 5]), torch.Size([10, 6]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape, actions.shape, action_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff42bce-255d-48b5-819f-798acbda4d23",
   "metadata": {},
   "source": [
    "Write a function calculate the loss of soft value function according to\n",
    "\n",
    "$J_V(\\psi)=\\mathbb{E}_{\\mathbf{s}_t \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(V_\\psi\\left(\\mathbf{s}_t\\right)-\\mathbb{E}_{\\mathbf{a}_t \\sim \\pi_\\phi}\\left[Q_\\theta\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)-\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\right]\\right)^2\\right]$\n",
    "\n",
    "(Equation 5 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a359a1c2-c474-4269-96ce-5988b4b91ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_of_soft_v(states, actions, action_probs):\n",
    "    entropy_of_policy = action_probs.log()\n",
    "        \n",
    "    q_expectation = torch.mean(q_function(states, actions) - entropy_of_policy)\n",
    "    \n",
    "    mean_square_error = (v_function(states) - q_expectation).pow(2) / 2\n",
    "    \n",
    "    loss = torch.mean(mean_square_error)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bb98c72-b881-40f8-bad9-bf7be6897e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.0669, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss_of_soft_v(states, actions, action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fe027-f830-4aeb-aeb2-ed668b8f9614",
   "metadata": {},
   "source": [
    "### Loss function of Q function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017e74a-6a7c-430e-9e68-1891bd6d74f7",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c8e90-e383-45f4-8c5b-e18a304c6b92",
   "metadata": {},
   "source": [
    "Given\n",
    "- `states`: a list of observations that the agent observed at each time step\n",
    "- `actions`: a list of actions that the agent took at each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf14659-8e9f-4722-9954-5c79c848b675",
   "metadata": {},
   "source": [
    "Write pseudocode for the calculate the loss of q-network according the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd481e-dd47-457f-b008-71864c904bcb",
   "metadata": {},
   "source": [
    "$J_Q(\\theta)=\\mathbb{E}_{\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right) \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(Q_\\theta\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)-\\hat{Q}\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)\\right)^2\\right]$\n",
    "\n",
    "with $\\hat{Q}\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)=r\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p}\\left[V_{\\bar{\\psi}}\\left(\\mathbf{s}_{t+1}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6f4cc-aeab-4c4e-9ea3-8e961b1d0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q = get_rewards(states, actions) + discount_factor * v_network(states+1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e817b9-7532-490e-93a7-830bc4f7be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_q = q_network(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372ab45-f8e5-487f-a7b7-637d4b4c551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ((predicted_q - target_q).pow(2) * 0.5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325e91e-cd3a-447b-9d4c-534ad9ab0827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
