{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://youtu.be/hlv79rcHws0\n",
    "# Proximal Policy Optimization (PPO) is Easy With PyTorch | Full PPO Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def generate_batchs(self):\n",
    "        n_states = len(self.states)\n",
    "        \n",
    "        batch_start = np.arrange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        indices = np.random.shuffle(indices)\n",
    "        \n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        return np.array(self.states),\\\n",
    "               np.array(self.actions),\\\n",
    "               np.array(self.probs),\\\n",
    "               np.array(self.rewards),\\\n",
    "               np.array(self.dones),\\\n",
    "               batches\n",
    "        # return batches\n",
    "    \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, n_observations, fc1_dim=256, fc2_dim=256):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dim, fc2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dim, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, fc1_dim=256, fc2_dim=256):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dim, fc2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-3\n",
    "CRITIC_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, n_actions, n_observations,\n",
    "        gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "        policy_clip=0.2, batch_size=64, max_steps=2048,\n",
    "        n_epochs=10\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        # TODO: add alpha, gamma\n",
    "        self.actor = ActorNetwork(n_actions, n_observations)\n",
    "        self.critic = CriticNetwork(n_observations)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "    \n",
    "    def remember(self, state, action, log_prob, value, reward, done):\n",
    "        \"\"\"Store transitions in one step\n",
    "\n",
    "        Args:\n",
    "            state (_type_): _description_\n",
    "            action (_type_): _description_\n",
    "            log_prob (_type_): _description_\n",
    "            value (_type_): _description_\n",
    "            reward (_type_): _description_\n",
    "            done (function): _description_\n",
    "        \"\"\"\n",
    "        self.memory.store_memory(state, action, log_prob, value, reward, done)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor([observation], dtype=torch.float)\n",
    "        \n",
    "        # action distribution from a given state\n",
    "        dist = self.actor(state)\n",
    "        # the value of the state\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "        \n",
    "        return action, log_prob, value\n",
    "\n",
    "    # def compute_advantage(self, rewards):\n",
    "    #     n_rewards = len(rewards)\n",
    "    #     advantages = np.zeros(n_rewards, dtype=torch.float32)\n",
    "        \n",
    "    #     for t in range(n_rewards - 1):\n",
    "    #         discount = 1\n",
    "    #         a_t = 0\n",
    "            \n",
    "    #         for k in range(t, n_rewards - 1):\n",
    "    #             pass        \n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            states, actions, probs, values, rewards, dones, batches = self.memory.generate_batchs()\n",
    "            \n",
    "            advantage = np.zeros(len(rewards), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(rewards)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(rewards)-1):\n",
    "                    a_t += discount*(rewards[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = torch.tensor(advantage)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states = torch.tensor(states[batch], dtype=torch.float)\n",
    "                probs = torch.tensor(probs[batch], dtype=torch.float)\n",
    "                actions = torch.tensor(actions[batch], dtype=torch.float)\n",
    "                \n",
    "                old_probs = probs\n",
    "                \n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "                \n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                \n",
    "                weighted_probs = prob_ratio * advantage[batch]\n",
    "                \n",
    "                policy_clipped = torch.clamp(prob_ratio, min=1-self.policy_clip, max=1+self.policy_clip)\n",
    "                weighted_clipped_ratio = policy_clipped * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_ratio)\n",
    "                \n",
    "                # TODO: this loss difference from the loss in paper\n",
    "                \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                \n",
    "                # TODO: where is this loss\n",
    "                critic_loss = (returns - critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                # TODO: where is this\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # actor_optimizer.zero_grad()\n",
    "                # critic_optimizer.zero_grad()\n",
    "                # total_loss.backward()\n",
    "                # actor_optimizer.step()\n",
    "                # critic_optimizer.step()\n",
    "        \n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 5\n",
    "alpha = 0.0003\n",
    "n_games = 300\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=128, bias=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_observations, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    n_actions=n_actions,\n",
    "    n_observations=n_observations,\n",
    "    batch_size=batch_size, n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = optim.Adam(agent.actor.parameters(), lr=ACTOR_LR)\n",
    "critic_optimizer = optim.Adam(agent.critic.parameters(), lr=CRITIC_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m agent\u001b[39m.\u001b[39mremember(state, action, log_prob, value, reward, done)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m n_steps \u001b[39m%\u001b[39m N \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     18\u001b[0m     learn_iters \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "Cell \u001b[0;32mIn[58], line 60\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epochs):\n\u001b[0;32m---> 60\u001b[0m         states, actions, probs, values, rewards, dones, batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mgenerate_batchs()\n\u001b[1;32m     62\u001b[0m         advantage \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(rewards), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     64\u001b[0m         \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(rewards)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mPPOMemory.generate_batchs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_batchs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     13\u001b[0m     n_states \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates)\n\u001b[0;32m---> 15\u001b[0m     batch_start \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marrange(\u001b[39m0\u001b[39m, n_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m     16\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(n_states, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64)\n\u001b[1;32m     17\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(indices)\n",
      "File \u001b[0;32m~/miniforge3/envs/gym/lib/python3.8/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 311\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrange'"
     ]
    }
   ],
   "source": [
    "for i in range(n_games):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob, value = agent.choose_action(state)\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        episode_score += reward\n",
    "        n_steps += 1\n",
    "        \n",
    "        agent.remember(state, action, log_prob, value, reward, done)\n",
    "        \n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    score_history.append(episode_score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
