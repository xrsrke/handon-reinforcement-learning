diff --git a/examples/07_ppo.ipynb b/examples/07_ppo.ipynb
index 62b1e95..f0a892c 100644
--- a/examples/07_ppo.ipynb
+++ b/examples/07_ppo.ipynb
@@ -67,7 +67,7 @@
     "    action = env.action_space.sample()\n",
     "    observation, reward, done, truncated, info = env.step(action)\n",
     "    # episodic_return += reward\n",
-    "    \n",
+    "     \n",
     "    if done:\n",
     "        observation, _ = env.reset()\n",
     "        episodic_return = 0\n",
diff --git a/examples/wandb/latest-run b/examples/wandb/latest-run
index 5bae36a..4ae7ca3 120000
--- a/examples/wandb/latest-run
+++ b/examples/wandb/latest-run
@@ -1 +1 @@
-run-20221224_123110-3jewyw8g
\ No newline at end of file
+run-20230105_072840-1fubbhhr
\ No newline at end of file
diff --git a/theory/06_soft_actor_critic.ipynb b/theory/06_soft_actor_critic.ipynb
index 24039e3..60bc772 100644
--- a/theory/06_soft_actor_critic.ipynb
+++ b/theory/06_soft_actor_critic.ipynb
@@ -210,7 +210,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "for i in range(n_steps):\n",
+    "for t in range(n_steps):\n",
     "    # Compute the reward for the current time step\n",
     "    reward = compute_reward(states[t], actions[t])\n",
     "    \n",
@@ -749,7 +749,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "loss = (predicted_q - target_q).pow(2).mean()"
+    "loss = ((predicted_q - target_q).pow(2) * 0.5).mean()"
    ]
   },
   {
