{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, mem_size: int, input_shape, n_actions: int):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        # memory counter\n",
    "        self.state_memory = torch.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = torch.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = torch.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = torch.zeros(self.mem_size)\n",
    "        self.terminal_memory = torch.zeros(self.mem_size, dtype=torch.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, new_state, reward, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    def sample_buffer(self, batch_size: int):\n",
    "        current_max_mem = min(self.mem_counter, self.mem_size)\n",
    "        \n",
    "        batch = torch.randperm(current_max_mem)[:batch_size]\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, new_states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a51d2d6d25395c24e0d12246d2018dcbf7cbc51d78bb42126dff68c94d75ef25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
