{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b7735e",
   "metadata": {},
   "source": [
    "Correct. The action distribution in the actor network is an unbounded Gaussian distribution, which means it can take on any real value. However, in practice, the actions must be restricted to a certain range or interval, such as between -1 and 1. To enforce this bound, the actor network applies an invertible squashing function, such as the hyperbolic tangent function (tanh), to the Gaussian samples to map them to the bounded interval.\n",
    "\n",
    "The change of variables formula is then used to adjust the log-likelihood of the bounded action distribution to account for the fact that the action is being squashed from an unbounded Gaussian distribution to a bounded interval using the tanh function. The formula for the log-likelihood has the form $\\log \\pi(\\mathbf{a} \\mid \\mathbf{s})=\\log \\mu(\\mathbf{u} \\mid \\mathbf{s})-\\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$, where $\\pi(\\mathbf{a} \\mid \\mathbf{s})$ is the density of the bounded action distribution, $\\mu(\\mathbf{u} \\mid \\mathbf{s})$ is the density of the unbounded Gaussian distribution, and $\\tanh ^2\\left(u_i\\right)$ is the Jacobian of the transformation from the unbounded to bounded action distribution. This adjustment ensures that the new bounded action distribution still has the same meaning or interpretation as the old unbounded Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd886c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17934ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02a66bb",
   "metadata": {},
   "source": [
    "The Jacobian of the transformation from the unbounded variable $u$ to the bounded variable $a$ is given by the diagonal matrix $\\operatorname{diag}\\left(1-\\tanh ^2\\left(u_i\\right)\\right)$, where $u_i$ is the $i^{\\text{th}}$ element of $\\mathbf{u}$. Since the Jacobian is diagonal, the log-likelihood of the bounded action distribution has the form $\\log \\pi(\\mathbf{a} \\mid \\mathbf{s}) = \\log \\mu(\\mathbf{u} \\mid \\mathbf{s})-\\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$. The term $\\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$ adjusts the log-likelihood of the bounded action distribution to account for the transformation from the unbounded Gaussian to the bounded interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f85512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59203c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7775819b",
   "metadata": {},
   "source": [
    "I apologize for the mistake in my previous explanation. The Jacobian of the transformation is used to transform probability distributions, as you correctly pointed out.\n",
    "\n",
    "To understand why we subtract the Jacobian of the transformation in this case, we need to understand the change of variables formula for probability distributions. This formula tells us how to transform a probability density function (PDF) from one random variable to another.\n",
    "\n",
    "Suppose we have two random variables, $X$ and $Y$, with PDFs $f_X(x)$ and $f_Y(y)$, respectively. Let $Y=g(X)$ be a function that transforms $X$ to $Y$. Then the PDF of $Y$ can be obtained from the PDF of $X$ using the following formula:\n",
    "\n",
    "$$f_Y(y) = f_X(x) \\left| \\frac{dx}{dy} \\right|$$\n",
    "\n",
    "where $x$ is a function of $y$, i.e., $x=g^{-1}(y)$.\n",
    "\n",
    "In our case, the random variable $Y$ represents the bounded action, and the random variable $X$ represents the unbounded action. The function $g(X)$ is the invertible squashing function (i.e., tanh). The Jacobian of this transformation is the absolute value of the derivative of $g(X)$ with respect to $X$, which is $\\left| \\frac{dy}{dx} \\right| = \\left| \\operatorname{diag}\\left(1-\\tanh ^2\\left(u_i\\right)\\right) \\right|$.\n",
    "\n",
    "Therefore, to transform the PDF of the unbounded action to the PDF of the bounded action, we need to apply the change of variables formula and subtract the Jacobian of the transformation, as shown in the equation:\n",
    "\n",
    "$$\\log \\pi(\\mathbf{a} \\mid \\mathbf{s}) = \\log \\mu(\\mathbf{u} \\mid \\mathbf{s}) - \\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d595e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b2e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f3d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f5bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd0856d",
   "metadata": {},
   "source": [
    "The determinant of a diagonal matrix is the product of its diagonal elements.\n",
    "\n",
    "For example, given a diagonal matrix $\\mathbf{A}$ with diagonal elements $a_{1}, a_{2}, \\dots, a_{n}$, the determinant of $\\mathbf{A}$ is given by:\n",
    "\n",
    "$$\\det(\\mathbf{A}) = a_{1}a_{2} \\dots a_{n}$$\n",
    "\n",
    "Note that the determinant of a diagonal matrix is non-zero if and only if all of its diagonal elements are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17678dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50752af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499aefb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddb145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d79b0d68",
   "metadata": {},
   "source": [
    "The Jacobian of a transformation is used to determine how the transformation affects the area or volume of the original space. When transforming from one distribution to another, we often want to preserve the area or volume, so that the new distribution still has the same meaning as the original one.\n",
    "\n",
    "In the case of transforming from an unbounded Gaussian distribution to a bounded interval using the hyperbolic tangent function, the Jacobian of the transformation is given by the diagonal matrix $|\\operatorname{det}(\\frac{d \\mathbf{a}}{d \\mathbf{u}})|^{-1} = |\\operatorname{diag}(1-\\tanh ^2(\\mathbf{u}))|^{-1}$. Since the determinant of a diagonal matrix is simply the product of its diagonal elements, the Jacobian is equal to the inverse of the product of the elements on the diagonal, which are $(1-\\tanh ^2(u_1)), (1-\\tanh ^2(u_2)), \\dots, (1-\\tanh ^2(u_D))$.\n",
    "\n",
    "Therefore, in order to transform from the unbounded Gaussian distribution to the bounded interval while preserving the meaning of the original distribution, we need to adjust the log-likelihood of the new bounded action distribution by subtracting the logarithm of the Jacobian of the transformation. This is what is done in the equation you provided:\n",
    "\n",
    "$$ \\log \\pi(\\mathbf{a} \\mid \\mathbf{s}) = \\log \\mu(\\mathbf{u} \\mid \\mathbf{s}) - \\sum_{i=1}^D \\log \\left(1-\\tanh ^2\\left(u_i\\right)\\right) $$\n",
    "\n",
    "This ensures that the new bounded action distribution has the same meaning as the original unbounded Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515d494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79172ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a288153",
   "metadata": {},
   "source": [
    "The term $J(\\pi)$ is a measure of the performance of a policy $\\pi$. It is an expectation over the distribution $\\rho_\\pi$, which is the distribution of states and actions that results from following policy $\\pi$. The expectation is taken over pairs of states and actions $(\\mathbf{s}_t, \\mathbf{a}_t)$ that are generated by following policy $\\pi$. The expectation is a sum over all time steps $t=0, 1, \\dots, T$. At each time step, the expectation is the sum of the reward $r(\\mathbf{s}_t, \\mathbf{a}_t)$ obtained at that time step and an entropy term $\\alpha \\mathcal{H}(\\pi(\\cdot \\mid \\mathbf{s}_t))$, which is a measure of the uncertainty of the policy at that state. The term $\\alpha$ is a hyperparameter that controls the relative importance of the entropy term. The entropy of a policy at a given state is defined as the expected value of the negative log probability of the policy at that state, and is a measure of the spread of the policy's distribution at that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f7805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92efbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a699ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03385084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c570e52f",
   "metadata": {},
   "source": [
    "The logarithm of the policy probability, $\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)$, measures the likelihood of taking action $\\mathbf{a}_t$ in state $\\mathbf{s}t$ under the current policy $\\pi\\phi$. It is a scalar value that is typically negative, since the probabilities of individual actions are usually small. In the context of reinforcement learning, the log probability of an action is often used to compute the gradient of the policy with respect to the policy parameters, as well as to evaluate the performance of the policy.\n",
    "\n",
    "In the equation $J_V(\\psi)=\\mathbb{E}{\\mathbf{s}t \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(V\\psi\\left(\\mathbf{s}t\\right)-\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}t\\right)\\right]\\right)^2\\right]$, the term $\\log \\pi\\phi\\left(\\mathbf{a}t \\mid \\mathbf{s}t\\right)$ is subtracted from the expected value of the Q-function, $\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)\\right]$, to form the soft value function, which is then compared to the value function $V\\psi\\left(\\mathbf{s}_t\\right)$. This comparison is used to update the value function parameters $\\psi$, so that the value function better approximates the soft value function. The motivation for using the soft value function, rather than the Q-function, is that the soft value function is a better estimate of the optimal value function, since it takes into account the entropy of the policy. This helps to ensure that the value function is not overly influenced by the current policy, and encourages exploration during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b3b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d476ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c505b589",
   "metadata": {},
   "source": [
    "The soft value function is a function that estimates the expected return from a state under the current policy. It is defined as\n",
    "$$V_\\psi(\\mathbf{s})=\\mathbb{E}{\\mathbf{a} \\sim \\pi\\phi}\\left[Q_\\theta\\left(\\mathbf{s}, \\mathbf{a}\\right)-\\log \\pi_\\phi\\left(\\mathbf{a} \\mid \\mathbf{s}\\right)\\right]$$\n",
    "where $\\pi_\\phi$ is the current policy, $Q_\\theta$ is the current action-value function, and $\\psi$ are the parameters of the soft value function.\n",
    "\n",
    "The term $Q_\\theta\\left(\\mathbf{s}, \\mathbf{a}\\right)$ measures the expected return from taking action $\\mathbf{a}$ in state $\\mathbf{s}$, and the term $\\log \\pi_\\phi\\left(\\mathbf{a} \\mid \\mathbf{s}\\right)$ is the log probability of taking action $\\mathbf{a}$ under the current policy. The difference between these two terms is known as the \"advantage\" of taking action $\\mathbf{a}$ in state $\\mathbf{s}$. The soft value function estimates the expected return under the current policy by averaging the advantage over all possible actions.\n",
    "\n",
    "The soft value function is used in the soft actor-critic algorithm as an intermediate step in the policy improvement process. It is trained to minimize the squared residual error between the soft value function and the expected return under the current policy, as shown in the equation you provided:\n",
    "$$J_V(\\psi)=\\mathbb{E}{\\mathbf{s}t \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(V\\psi\\left(\\mathbf{s}t\\right)-\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\right]\\right)^2\\right]$$\n",
    "Here, $\\mathcal{D}$ is the distribution of previously sampled states and actions, or a replay buffer. The gradient of this equation can be estimated with an unbiased estimator to update the parameters $\\psi$ of the soft value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaff0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfb4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce85f99f",
   "metadata": {},
   "source": [
    "The soft value function is a function that estimates the expected return of the current policy at a given state. It is defined as:\n",
    "\n",
    "$\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\right]$\n",
    "\n",
    "Here's an explanation of each part of this function:\n",
    "\n",
    "$\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}$: This is the expected value over all possible actions at time step t, where the probability of selecting each action is determined by the current policy $\\pi\\phi$.\n",
    "\n",
    "$Q_\\theta\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)$: This is the action-value function, which estimates the expected return of taking a particular action at a particular state. It is parameterized by $\\theta$.\n",
    "\n",
    "$-\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)$: This term is known as the entropy of the policy, which measures the amount of randomness or uncertainty in the policy. By subtracting the entropy from the action-value function, the soft value function encourages exploration, as the entropy will be high for actions that the policy is uncertain about and low for actions that the policy is confident about.\n",
    "\n",
    "Overall, the soft value function estimates the expected return of the current policy by taking the expected value of the action-value function over all possible actions, weighted by the probability of selecting each action according to the current policy, and subtracting the entropy of the policy. This encourages exploration by rewarding actions that the policy is uncertain about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fe226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0ec4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7de71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f0f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a860ec",
   "metadata": {},
   "source": [
    "The term $\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)$ is the negative entropy of the policy at time $t$. In probability theory and information theory, the entropy of a random variable is a measure of the uncertainty associated with the variable. For a discrete random variable $X$ with possible outcomes $x_1, x_2, \\dots, x_n$ and corresponding probabilities $p_1, p_2, \\dots, p_n$, the entropy is defined as\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^n p_i \\log p_i$$\n",
    "\n",
    "where the logarithm is taken in base 2. The entropy of a continuous random variable is defined similarly, but with the sum replaced by an integral.\n",
    "\n",
    "In the context of the soft value function, $\\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)$ is the probability of taking action $\\mathbf{a}_t$ in state $\\mathbf{s}_t$ under the current policy, and the negative entropy is a measure of the uncertainty or randomness in the actions taken by the policy. The entropy term is included in the soft value function to encourage exploration and discourage premature convergence to a suboptimal policy.\n",
    "\n",
    "So, the expression $\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\right]$ is a measure of the expected value of the action-value function, minus the negative entropy of the policy. This combination is used as a measure of the overall performance of the policy, with the action-value function representing the expected reward and the entropy term representing the exploration bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c0e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5d62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f6cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08338a91",
   "metadata": {},
   "source": [
    "The expression $\\frac{\\exp \\left(Q^{\\pi_{\\mathrm{old}}}\\left(\\mathbf{s}t, \\cdot\\right)\\right)}{Z^{\\pi{\\mathrm{old}}}\\left(\\mathbf{s}_t\\right)}$ is a probability distribution over the action space at state $\\mathbf{s}t$. It is referred to as the \"old\" policy because it is based on the current estimate of the optimal behavior under the old policy. The new policy, denoted by $\\pi{\\mathrm{new}}$, is chosen to minimize the Kullback-Leibler (KL) divergence between the new policy and this target distribution. The KL divergence is a measure of the difference between two probability distributions, and minimizing it ensures that the new policy is as close as possible to the target distribution, which is based on the current estimate of the optimal behavior under the old policy.\n",
    "\n",
    "By taking the exponential of the Q-function, the distribution is made more peaked around the actions that have high Q-values, which are the actions that are expected to have high reward. The normalizing constant $Z^{\\pi_{\\mathrm{old}}}\\left(\\mathbf{s}_t\\right)$ ensures that the resulting distribution is properly normalized, i.e., it sums to 1.\n",
    "\n",
    "The new policy is then chosen to minimize the KL divergence between itself and this target distribution, which ensures that the new policy is as close as possible to the current estimate of the optimal behavior under the old policy. This process is repeated until the policy has converged to the optimal behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    " the target distribution, which is based on the current estimate of the optimal behavior under the old policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e81da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b14dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1612563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8366,  0.2022, -0.3334,  1.3292], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the action-value function, Q\n",
    "Q = torch.nn.Linear(in_features=10, out_features=4)\n",
    "\n",
    "# Generate a random state tensor\n",
    "s = torch.randn(10)\n",
    "\n",
    "# Compute the action-values for the state\n",
    "q = Q(s)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5314f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the partition function, Z\n",
    "def Z(s):\n",
    "  q = Q(s)\n",
    "  return torch.exp(q).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeee02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distribution over actions\n",
    "def distribution(s):\n",
    "  q = Q(s)\n",
    "  return torch.exp(q) / Z(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05965096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0704, 0.1990, 0.1165, 0.6141], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the distribution over actions for the state\n",
    "p = distribution(s)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a62519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "208d7740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1353)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(-2).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84c2568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0089e-43)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(-99).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d6c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c92292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb821afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4cc3eb",
   "metadata": {},
   "source": [
    "$Q^{\\pi_{\\text{old}}}\\left(\\mathbf{s}t, \\cdot\\right)$ is the action-value function under the old policy $\\pi{\\text{old}}$, and its output is a scalar value. It represents the expected return of taking action $a$ in state $s_t$ while following policy $\\pi_{\\text{old}}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38325b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2aba99-b1ed-4b38-8100-9e3e1dbf6ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c7137-d1e1-418a-bcab-2a8064352004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40ed85-9db6-45b4-9e8e-ae4545ff827c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2332d65-d79b-4273-af42-3d6e6d29991d",
   "metadata": {},
   "source": [
    "$1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36660b6f-1a4f-46f5-b993-8649af6d6cee",
   "metadata": {},
   "source": [
    "The soft value function is trained to minimize the squared residual error between the predicted value of the state $V_\\psi(\\mathbf{s}t)$ and the expected value of the state-action pair $\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q_\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}t \\mid \\mathbf{s}t\\right)\\right]$. This expected value is calculated using the current policy $\\pi\\phi$ and the current estimate of the state-action value function $Q\\theta$. The error is squared to ensure that the gradient of the loss function is always positive, making the optimization problem well-behaved. The expectation is taken over the distribution $\\mathcal{D}$ of previously sampled states and actions, or a replay buffer. The function approximator for the soft value function is trained to minimize this loss using stochastic gradient descent.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175ece8-c607-47df-80e9-f28ca85a3a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccfce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d189740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60531162",
   "metadata": {},
   "source": [
    "Not exactly. In the Soft Actor-critic (SAC) algorithm, the objective is to maximize the entropy of the policy while also minimizing the error between the soft value function and the expected return under the current policy. The expected return under the current policy is given by the expression\n",
    "$\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}t\\right)\\right]$, which is known as the soft Q-value.\n",
    "The term $\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}t\\right)$ is subtracted from the soft Q-value in order to encourage exploration by increasing the entropy of the policy. This is because the entropy of the policy is given by\n",
    "$\\mathcal{H}\\left(\\pi\\left(\\cdot \\mid \\mathbf{s}t\\right)\\right) = -\\sum{a \\in \\mathcal{A}} \\pi\\phi\\left(a \\mid \\mathbf{s}t\\right) \\log \\pi\\phi\\left(a \\mid \\mathbf{s}t\\right)$.\n",
    "If the policy is deterministic, i.e., $\\pi\\phi\\left(a \\mid \\mathbf{s}t\\right) = 1$ for a single action $a$ and $\\pi\\phi\\left(a \\mid \\mathbf{s}t\\right) = 0$ for all other actions, then the entropy of the policy is zero. Subtracting $\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)$ from the soft Q-value encourages the policy to be more stochastic and therefore increases the entropy of the policy.\n",
    "\n",
    "The objective of the SAC algorithm is to minimize the loss function $J_V(\\psi)$, which measures the squared error between the soft value function $V_\\psi\\left(\\mathbf{s}t\\right)$ and the expected return under the current policy $\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q_\\theta\\left(\\mathbf{s}_t, \\mathbf{a}t\\right)-\\log \\pi\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right)\\right]$. The aim is to find the set of parameters $\\psi$ that minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b13a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb450e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50e04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0ddd4d",
   "metadata": {},
   "source": [
    "To see how the entropy of the policy affects the loss in the equation you provided, let's consider two different policies with different entropies.\n",
    "\n",
    "For simplicity, let's assume that both policies have the same state value function, $V_\\psi(\\mathbf{s}t)$, and action-value function, $Q\\theta(\\mathbf{s}_t, \\mathbf{a}_t)$. Let's also assume that the distribution of previously sampled states, $\\mathcal{D}$, is the same for both policies.\n",
    "\n",
    "Policy 1 has a high entropy, while policy 2 has a low entropy.\n",
    "\n",
    "For policy 1, let's say that the entropy of the policy is given by $\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) = 3$. Plugging this into the equation for the loss, we get:\n",
    "\n",
    "$$J_V(\\psi) = \\mathbb{E}{\\mathbf{s}t \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(V\\psi\\left(\\mathbf{s}t\\right)-\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)-3\\right]\\right)^2\\right]$$\n",
    "\n",
    "For policy 2, let's say that the entropy of the policy is given by $\\log \\pi_\\phi\\left(\\mathbf{a}_t \\mid \\mathbf{s}_t\\right) = 1$. Plugging this into the equation for the loss, we get:\n",
    "\n",
    "$$J_V(\\psi) = \\mathbb{E}{\\mathbf{s}t \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(V\\psi\\left(\\mathbf{s}t\\right)-\\mathbb{E}{\\mathbf{a}t \\sim \\pi\\phi}\\left[Q\\theta\\left(\\mathbf{s}_t, \\mathbf{a}_t\\right)-1\\right]\\right)^2\\right]$$\n",
    "\n",
    "In this case, we can see that the loss for policy 2 is larger than the loss for policy 1 because the entropy of policy 2 is smaller. This is because the negative sign in front of the entropy term causes a larger entropy to result in a smaller loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422a74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
